"""
Backend selector for Whisper transcription.

Dynamically selects between PyTorch and MLX backends based on:
1. Environment variable SIGNALHUB_USE_MLX
2. Platform detection (macOS with Apple Silicon)
3. Availability of MLX libraries

This provides a single interface for the application to get the appropriate
Whisper processor without needing to know which backend is being used.
"""
import os
import platform
import logging
from typing import Union

logger = logging.getLogger('signalhub.whisper_backend_selector')

# Try to import both backends
_PYTORCH_AVAILABLE = False
_MLX_AVAILABLE = False

try:
    from .whisper_processor import WhisperProcessor
    _PYTORCH_AVAILABLE = True
except ImportError as e:
    logger.warning(f"PyTorch Whisper backend not available: {e}")

_mlx_probe = lambda: False  # type: ignore

try:
    from .whisper_processor_mlx import WhisperProcessorMLX, is_mlx_available as _mlx_available_probe
    _MLX_AVAILABLE = _mlx_available_probe()
    _mlx_probe = _mlx_available_probe
except ImportError as e:
    logger.debug(f"MLX Whisper backend not available: {e}")


def should_use_mlx() -> bool:
    """
    Determine if MLX backend should be used.
    
    Decision criteria:
    1. If SIGNALHUB_USE_MLX=1, use MLX (if available)
    2. If SIGNALHUB_USE_MLX=0, use PyTorch
    3. If not set, auto-detect:
       - Use MLX on macOS with Apple Silicon (if available)
       - Otherwise use PyTorch
    
    Returns:
        True if MLX should be used, False for PyTorch
    """
    # Explicit environment variable takes precedence
    env_mlx = os.getenv("SIGNALHUB_USE_MLX", "").strip()
    mlx_available = _mlx_probe()
    global _MLX_AVAILABLE
    _MLX_AVAILABLE = mlx_available
    
    # Debug logging to diagnose environment variable issues
    logger.info(f"🔍 [MLX ENV CHECK] SIGNALHUB_USE_MLX='{env_mlx}' (from os.getenv)")
    
    if env_mlx == "1":
        if mlx_available:
            logger.info("MLX backend explicitly enabled via SIGNALHUB_USE_MLX=1")
            return True
        else:
            logger.warning("MLX backend requested but not available, falling back to PyTorch")
            return False
    
    if env_mlx == "0":
        logger.info("MLX backend explicitly disabled via SIGNALHUB_USE_MLX=0")
        return False
    
    # Auto-enable MLX on Apple Silicon (when env var not set)
    if platform.system() == "Darwin" and platform.machine() == "arm64":
        if mlx_available:
            logger.info("Auto-selecting MLX backend for Apple Silicon (3.25x faster)")
            return True
        else:
            logger.debug("MLX not available on Apple Silicon, using PyTorch")
            return False
    
    # Default to PyTorch for other platforms
    logger.debug("Using PyTorch backend (default)")
    return False


def get_whisper_processor(model_name: str = "tiny") -> Union['WhisperProcessor', 'WhisperProcessorMLX']:
    """
    Get the appropriate Whisper processor based on environment and availability.
    
    Args:
        model_name: Whisper model size (tiny, base, small, medium, large)
        
    Returns:
        WhisperProcessor or WhisperProcessorMLX instance
        
    Raises:
        RuntimeError: If no backend is available
    """
    use_mlx = should_use_mlx()
    
    if use_mlx:
        if _mlx_probe():
            try:
                logger.info(f"Creating MLX Whisper processor with model: {model_name}")
                return WhisperProcessorMLX(model_name=model_name)
            except RuntimeError as err:
                logger.warning(f"Failed to initialize MLX backend ({err}), falling back to PyTorch")
                _MLX_AVAILABLE = False
        else:
            logger.warning("MLX backend not available, falling back to PyTorch")
        use_mlx = False
    
    if not use_mlx:
        if _PYTORCH_AVAILABLE:
            logger.info(f"Creating PyTorch Whisper processor with model: {model_name}")
            return WhisperProcessor(model_name=model_name)
        else:
            raise RuntimeError("No Whisper backend available (neither PyTorch nor MLX)")
    
    raise RuntimeError("Failed to create Whisper processor")


def get_backend_info() -> dict:
    """
    Get information about available backends and current selection.
    
    Returns:
        Dictionary with backend availability and selection info
    """
    return {
        "pytorch_available": _PYTORCH_AVAILABLE,
        "mlx_available": _mlx_probe(),
        "should_use_mlx": should_use_mlx(),
        "platform": platform.system(),
        "machine": platform.machine(),
        "env_mlx_flag": os.getenv("SIGNALHUB_USE_MLX", "not_set"),
    }


# Global processor instance (lazy-loaded for backward compatibility)
_global_processor = None


def get_global_whisper_processor():
    """
    Get or create the global Whisper processor instance.
    
    This maintains backward compatibility with code that imports
    whisper_processor directly.
    
    Returns:
        Global WhisperProcessor or WhisperProcessorMLX instance
    """
    global _global_processor
    
    if _global_processor is None:
        # Determine model name based on existing config
        # Default to 'base' for compatibility with existing code
        model_name = "base"
        
        # Override with 'tiny' if MLX is being used (for performance)
        if should_use_mlx():
            model_name = "tiny"
        
        _global_processor = get_whisper_processor(model_name=model_name)
        logger.info(f"Global Whisper processor created: {_global_processor.__class__.__name__}")
    
    return _global_processor
