
Yeah. So, for example, this is the total number of tests that is present with the application modules Luis and it D. For example, if you see the octane and these are the list of tests that we have, if we go to them we are application module. Here also we have various tests, but whichever test is having this kind of application module, Louis goal and sanity, then those tests will be retried by this particular API that I have given. Dislike just waiting. So, if you see the APA, whichever product area is having the Louis and also the sanity, I'm just pulling out all such test cases. If I give sanity alone, then it is pulling all the sanity test for example, this this particular test is having the sanity application module right. So similarly bestie in the AP test, here also there may be some sanity that's around Okay, not here, but if you see the suit, and all here also, I'm having the application module sanitary and in loci also, I'm having the application module 70, right. So it is pulling all the test cases that is associated with application modules and so that is why I was for this this demo purpose. I tried to pull out only the tests that are having up sanity application modules, and there are totally 27 such tests. So how I came up with this number 27 is like, initially, I have called this API, and it would have returned me a similar JSON that I showed you in the previous panel. So in that panel, I'm just filtering that panel with with this condition, so that it's not emitted. UDF can be true or is automated, UDS can be false. So, out of the total number of JSON values that we have, it will filter only the rows which which satisfies this condition. So if you see this panel, as well panel, we have like, totally around some twins. Let's say we have some Tito's and they start admitted IUDs will either have false or they'll have truth. So basically, the this APA which I called and this condition will filter me all the all the rows that I have, so it will give me the total number of rows. So here I have mentioned the condition as either it can be true or false. So it will treat me all the rows that I have. So I have summarized it using the count function. So that is why I'm getting the count here. 27 And we can also use the table view initially to get to understand whether we are able to properly get the data functionalization. So if we are in the table view, this would simply the output of this first query. And I forgot to mention that I have used the Infinity data source to call this REST API and also process the information. And the second query is the same. Same API, we are calling the same API. But in the filter condition, I'm just filtering it with its automated is equal to true. So whichever rows are having which is not emitted equal to true, that will be filtered initially and then I'm counting it so it is joining me then so 20 sets and query see is also the same, but with the condition is automated is equal to false. So I'm counting it so it is giving me one so if after I've got these three values, initially 3d, like a very A, B, C result, so I wanted a different name to it. And also I wanted to concatenate all the pieces together to visualize the data like this together. So far that I have used to transformation on transformation is concatenating. All the results of a B, C or D three queries and and then naming the results of the queries. By default it will be like a dot summary, P dot summary and C dot summary but I have renamed them like this application module so that it will give me a clearer picture of what I'm trying to filter it and what what is the data that it is actually posting in the visualization. So I'm able to do this using the Infinity data sources. Um, I think now are you having any visual spoilers if
yes, this is okay. But for everything we have to make a Korean Oh, I'm sorry. So what I was saying is for all the data here, but there are three data we are displaying it, total taste, and then automated not automated for each of the things we are coding separately. But instead, okay.
Yeah, yeah, at that point I was trying to explain, I was going to explain it. So we are using the same query, like I'm just pulling out the disk, if I'm just removing it. If I remove it, it will be like this. All the three queries are returning data like this only. But I am in each query. I'm just trying to get the result that I want for example, I want the total test. So I'm in a inquiry, I'm just pulling out the total recess. So I told him this, and in very, the I'm just doing the same. I'm requesting the same APA. But in the processing of the query, I'm just changing it so that I can get the automated tests to similar similarly, in the SE C's are the same ad, but processing is different. So in all the three ways I'm calling the same API, nothing is changed, but only the processing of the key is changed to get the desired result that we want for visualization. By but, as you said, this is one I'm kind of in one dish, like we have to query the same API three times to get the one. I mean, if there is a way in which we can query a simple API, but we can pass it in three different ways, then then I need not do this right again, and again. I will not do this. Right. Correct. So
that's yeah. Sorry, sorry. Yeah.
Yeah, maybe there should be aware but maybe for the feasibility thing, this should be okay. But when we are actually implementing it, we can dig deeper into that and optimize the same
Correct correct perfect perfect Yeah, because my I think my what I'm thinking is, you pull the we make the API request, get the data and then dump it in dB. From the DB whatever the way you want to create, like, you know, whether receipts automated equal to true or false, no or both are true. Either one of them is true or false kind of stuff. The way you're coding it, and then pull the data and then rendering it there. So that will make no instead of that will avoid making redundant calls API calls.
Yeah, you're getting your point, but that kind of squaring is also available here. itself itself, is it I mean, formed for this kind of rescue a thing. I don't think we need to go and store it in flux dB. I mean, just from whatever I tried, I'm just telling maybe you are you can also be right but I'm just telling from what I have tried to hit Yeah. So here, instead of this one, we can also use implies query languages also there. Okay. So from that also, it can for example, we can give a query here and but the thing is, I think we will be achieving the same thing what I have here,
okay, so you're saying that instead of storing it in data while get the parts that while parsing the data itself, we could do all these things. Okay. Okay. Understood.
Passing it tells is then like kind of using varying only what I have done here, right. So this is actually I'm giving it as text here in this text box, but in turn, it is actually queering the results and putting the technique like number 27,
correct, correct. But if we want to do more calculations into it, right, so in this case, so many things are very straightforward, like total test, passed or many fail that kind of stuff. But if we want to know like, you know, in that particular feature wise in gluey sanity Oh monitors the past in regression on monitors to pause our that component wise if you want to break it up. I'm just telling it I don't know whether we'll be using it in real time or not. If there is anything if you if you want to add some kind of calculations or if you want to do some process that informations will that be how how easy it would be like you know Okay, so that's fine. So those are like, you know, maybe the second step, third steps I understand this is just a feasibility test for the idea is whether we are able to create and then proceed and then displaying it. So far that is this is more than enough. But when we have I'm just trying to visualize because we know that it's quite possible now. What I'm trying to see is I'm trying to foresee what would be the next two major hurdles, we may be facing it. If you have a complete dashboard when you have a loaded dashboard and a suite dashboard, and what are things we will be displaying it right
now itself I would come up with some kind of To Do List okay. So I also made a documentation page because we may be losing track of these links and everything correct. So whatever I have done I have just documented it here. So I have told you most favorite thing of this but only thing is these three points we can discuss. For example. We have the fields returned from the API. But the correctness of this data I have not very faithfully for example, this all is automation. UDL is sort of an activity of is written by octane REST API. But I'm not seeing any, any field that I can map in this UI related to this. I'm not sure how it is deriving the value for this automated UDF but it is that from
Yeah, yeah, it is there is one year so that is no not that one. So that is one other field. So that is a use automated one. So let me let me open that 30 Probably you're not able to see that. Dashboard one. Give me a second. Let me open that. I will show that. If you want to have a sample. No. You just noted on one or two sample test cases. We can just verify whether it is true. or false. One with true and one route false. Yeah, so
again, not be any mistake. No, you're
right. But yeah, you're right. You're perfectly correct. So that should be the way we are to do it. So first one is the artwork one I'm just sharing my screen
maybe I'm gonna stop sharing.
Oh, yeah. And so you see this this one is automatable one that is a field.
Okay, so this automated is there is
automate is automated. Is it that is? Do you have any false one is automatable is automated false? Yeah, probably this case is automation state. I don't know.
Automation stayed automation status. All those are also available as part of fees returned in the PA. That's where I got confused and even asked the people about it. But I think I have to remind him again,
you see anything
false. I am checking it again. Shiro basic component. Tests number is 1841978
Louis regression.
And Louis I think it's not a Louis. So it is a sweeter
your basic component is similar You only
should be sorta Louis, can you check?
Yes, taking a Euro component, basic B or D validation
plus one plus one or need 4978
Okay, so that's the suit one. So if we just go to Basic, so you thought
okay, here it is, like four outs for that. And is it? Yeah, it is false or, or that is reading false for it. But I got confused like it is named as his automated UDF and not is automatable got confused.
Know that the your confusion is correct. So you should have a confusion so that we can double check it.
I'm also not saying that field as you said you are only
I know I know. So that is this is a one I'm also checking out the euro
and check if you are someone that knows.
Some other false not false. Yeah.
With tons of basic 219 Double one.
Just one second look like it got disconnected. Okay, okay. Sorry for that. I hope I'm losing interconnections.
Did you which one you're seeing it?
Unlike anything it's not in this it should be in New insanity. Okay, please sanity Are you efficient with unknown set that means?
Oh no, sir. Is it okay?
Okay. Got that one to one and one.
Right. Yeah, it should be no,
false it has returned false.
But it see is automatable should be actually this is wrong. It should be yesterday. Because automation state is automated. Automated, maybe what I will do, Kenny Karina will be able to Karina
Yeah, I will be able to, can I make this change, change, change and save it? Yeah, I say thank you for this. I'm not
sure whether it will be quickly reflected but you can just check it.
I just clicked the refresh Dashboard button and it refreshed it is now true.
Okay. So this is the one it is taking actually that
Yeah. should
or should we take this guy automation state?
Yeah, we should be checking for automation state.
So then, I mean, it's like a misnomer. It is named wrongly this way. So automatable UDF card is automated doing it
originally got one more doubt also. So you are able to carry all this information is like the study test type test priority testing. Tool type thing. Oh cool. Okay.
That is an explained Balaji. So, first of all, we have to check the correctness of the data that is written by the APA and whatever there is in obtain UI
that you wanted one second virginica, sorry to interrupt so that we updated in confidence, like you know, step by step, the correctness of the data to be checked. That's one. Yeah, that's correct. That's correct. And then what will be the second one?
Second one is like, depending on our requirement what we have to show in the dashboard, we have to maybe making changes in 10. Things we have to update some things. For example, if I want all the Louise and t tests alone, if I want the Lewis and t tests alone, then we have to maybe tag all the noise and status with both the application modules Louis and insanity, so that I will be getting them properly. The account
what is it can it can
so here I am having in the root folder of the UI, are you seeing the screen off? Anyway? Correct. So in the root folder of this, Louis, when I click I'm seeing the tests which are having the application module Louis regression Louise and to thing right. But if I clicked the sanity folder alone, these are actually the sanity tests of Louis Right. So here I'm seeing some applicate exam tests with application module Santi alone, but some other with like Louis and sanity. So for example, if I'm putting the if I'm having a query parameter a sanity it will return the sand all the tests, which have the application module sanity, so I won't be able to know which are the Louis sanity alone or that maybe I have to update it is all the Lewis entry test application modules Louis semicolon sanity, so that the count will be properly reflected in the dashboard. No.
Yeah, I think so. Before the to know before we get into this question, one other questions right. So you upgraded this Louis sanity, right? I'm just a little doubtful of this approach. Like when sorry, every folder in the left side, every folder. Every folder will have a kind of ID right. So we are we are going to do I don't know whether we have a separate ID for Lu Yi inside that sanity and regression also we have a separate ID that I don't know, if we know that Id can we carry all that test case inside that
folder? This ad is one
I don't know that? Maybe maybe not.
shouldn't be because it is changing and
changing right? Yeah. So you since we know this ID can we just read the test cases name or applications model inside that. So that we just keep only the sanity? We just
yeah, we I will check. Like if there is any query parameter that sought so yeah, Jen. Yeah. You're right managing so we can check for the mentors that we can query using the existing information. Or if that is not working. If for example, let's say we will not be able to query using this ID then we can change the information accordingly. Obtain UI update the fields, correct so that we are seeing proper information in the dashboard. Yes, yes.
Yeah. So in that way, we don't have to modify that existing one. We just try to leverage that. If things are not working, and then we can reconvene maybe next week or on between something we can modify it and then accordingly work on that.
Yeah. So yeah, if only needed,
only needed. Yeah. Only needed.
Yeah. Never needed we can get to because sometimes we might have we have already seen such cases. Right even back automated the test case, but would not have changed the automation state, change the district name and all here properly. So we have to always keep up the information in octane UI so that it is giving us correct users in the dashboard. That is what is the basic idea?
Yeah, so the reason why you're seeing is see now at least in our product, basically load Louise suite and all from the day one we were very particular of all these names, application modules. know all these things. I'm pretty mature. But you can't expect the same thing in other projects. This one, so if you are going directly going to application module and then rolling on that one, right? That will that should they may the developers has to be developers when the QA team has to do manually on in octane, that would be very difficult, it may they may take some time in updating the models so So instead, the more we rely on whatever the way it is, we can handle it through your script. So in that way it will be it will reduce the dependency on know the QA team to do modify things in in octane. Just trying to see the way the list rework from QA team, the more we can handle it through programmatically. So that will reduce a lot of headache here.
No, don't mean anyway, I have this though. Wait, some of this desta present in the Louis output folder and some of them are,
shouldn't be. When you just select the UI. You shouldn't have any test case idea is if you select sanity, that test case should be present in sanity. Even for me it was wondering because you You told that you also updated some Louis and all right, the moment you added Louis Collins regression, then the particular this case will be shown in Louis folder also.
Yeah, correct. Correct. I did not change anything in the UAE. What I'm asking is here, I mean, these are the totals sanity tests that we have within the audit within the Sandyford or whatever we have is the Louise and t test correct, but some of them are seen outside also because because of this only right Correct. That is what I'm asking why some of them are only seen outside and some of them are not.
Ideally, we should not have the know if we just click the sanity in under the Louis so in application models, Louis colon, Louis semicolons and it is the right but I don't know how it has come. I told them to keep it one only sanity I'm surprised to see that sanity here. Because if I had only sanity under sanity only these test cases will be written. Correct, correct. In but even for me it is surprised because we have not touched this for a while actually. I don't know. Okay, we need to check that. That's the case. You meanwhile, we also do some kind of research right. And if something to be modified, we will do it in advance. If we just do it for once. Yeah, the idea Yeah, you can keep it the more more we can handle it through. program would be a better than no handling it in depending on this octane work there's I am just thinking in that way, but we can double check with the sham what is thinking as well.
Like for returning the old ways to thing and all this is fine. We can have some we can have like a workaround. For example, instead of getting the application modules we can just get it from Heidi we can take there is a way to do that. But for other information we have to be up to date.
So, technically, ideally, expectation is that ID and automation stay descriptive name. No. The testing to type testing type and application models, all everything to be up to date. There shouldn't be any discrepancies.
Because whatever we have no water we have in this UI only is returning they're returning Oh,
that's true. That is proof. Let's see. But especially this application models is the one that people are using different different approaches. Some of them is reflected in all other places as well. So especially the application models is the one I don't want to dependant on that. But either way, let me know how things to be modified based on that we will modify that application models.
Yeah, once we have like kind of a clear requirement, we can definitely correct upon this UI thing. Correct? Correct. Yeah, the last point is like, yeah, the same as I told you, so I have to use the same APA tries to process it differently to get the references that I want initialisation so we have to check if there is any way to optimize this. And also I have
one more thing. Sorry, was the third one right, you mentioned about the Infinity data source. Is it. I mean, are we trying to use a infinity data source only infinity data source in the sense infinity dB, or it's a different one?
It's not related to influx, okay. So different, a tool kind of tool, which is used to mean get the response from media VAs and
fostered Okay, okay.
Like from for influx dB, we have a data associated flux data source, which will be helpful in getting data from influx dB and it will help us to pass it right Correct. Similarly, some person has written this data source to I'm also mentioned the links of those data so should be here. But I find the data says okay, so please go through this. I set up the JSON only because we have the JSON data. And so I have used some of the things that are even here. And then we also have mentioned here that letter to this data source. And that they have discussed things like what are the I mean, some people have asked for this, why should I be wearing the same query multiple times and all? So the developer, the one who developed it have answered it. But as of now, I just went with this to just check the feasibility, but later when we are actually implementing it, we have to try to optimize it. And there are also so many other data sources that can maybe do the same thing like getting the API response and able to grasp it. Um, those are also given various other data source ideas also here I have given it here. So why actually will actually implementing it if there is a nice they're good options, we can do the idea why not going through all the ideas is like, I was spending so much time in checking the feasibility itself. And like, when we are actually implementing it, we can check for the best available resource to do the expected job right. So now, I just tried to check the feasibility. That is why I did not like research all the various options available. Maybe if we are actually implementing it, we will have the support of a team also. So each of us can try the various data sources available. Each of us can pick more than one data source and try the requirement and we can discuss the pros and cons and we can come up with a solution. So that is why I thought, hey, let's not waste too much time on deciding which one I have to use. Correct?
Correct. Yeah, that's correct. Yeah. The only confusion I was having is infinity data source. See our REST API we are calling it return that response and return the response in JSON format. While we are making the request itself we can also request in which format we need the data. It is retained in JSON format, and then but and then using Grafana we pass it and then showcasing it, just trying to understand what the role of infinity data source is doing it, but as you said, but at least now we are able to do this. Since it is also a flexibility, check, read visibility check, right so that's fine. I will also no dig deep into that see if there is something what is the role of infinity data source in this one is going to replace this influx DB or it is just going to be no it is just going to be needed for to pass the data and then showing it in Grafana or something rough on on need the data from that influx data infinity data source to showcase it. Yeah, so either way is fine. So so far, it is good. Yeah.
So this is the Data Source Configuration. Here only we will be providing the octane secrets username password that it should be using. That they should be using to query the CPA and the requester is with APS. Yeah, this is the local Smith's nothing but the this host name that I have given there. Okay, which will be used for querying the REST API. And yeah, we just finished really saved this one. And then after that, every time when you put a URL here of the actual REST API, it will use that basic authentication to authenticate that request and it gives us the response Sure, sure. So so all these things I suppose as much possible I have documented here. So please go through it and also let me know.
I will I will also go through them. We'll discuss that Yeah. Sometime. We don't have to wait till next week in between also, if you have something I will just let you know. Yeah.
So one one last thing. Um, so here it says, I think we can set the refresh time also right. Now I have not set anything but usually we can set this out. I mean, how frequent it should refresh the dashboard stuff, it will do, right.
I didn't set the time because But ideally, yes, that should be is but
I think it is possible. I have not gone through the options for it. So what I was trying to tell that is because this whenever we refresh on it, now you see right it is it has refreshed doesn't understand zero, because you change that with turn on setting it is 27 cents on zero now, right? So, um, this this API is calling and whenever it is refreshing, it is actually pulling the latest data up anyway that we have correct. In this case, if we use the influx dB, then it will how it will work. I'm not getting it every bit getting confused. It will get the data and put the JSON in flux DB and interest upon interest a big growing right. So anyway, it this itself is giving me the current data. Yeah. So why we shouldn't be going with this.
Oh, yeah. So that's what I was saying. Right. So as long as you're able to the data, whatever we are getting it if we are not going to do any process, we are not going to do any process. We are good with this. But in case if we want to process something, right, process the data and then so that time, I'm not sure how it is going to helpful here the current method. So I don't know because I have not done this method. So if you're able to process the information, and then so then it may be useful if you can store the data somewhere and then fetch the data and process it and then store it back. So we write your code somewhere, processed the data and storing it in one place and then displaying it. And generally we store the data only for the historical if we want to find that trend and all right. I want to see if our last from last one month or last three, six months. No monthly it's the number of test cases are increasing it right. How we are going to get that trend, like historical data how we are going to get if you're not storing the data right, okay. So that parsing directly fits the query the data and then displaying it in dashboard is good. Unless if you want only the real time monitoring that is good, but if you want to have a historical, past history also if we need or if you want to come up with that no trending graph chart to kind of that's what sham also will be thinking right? So in that case, we may need to store the data we just not only to know render it, we just want to store the data, and then we want to display it
correctly. So okay, got it. So, but the simple use case may not may not do that, but for some other things if you want the history of data then we may have to
correct correct so my confusion. Now, my confusion is the Infinity data source is the DB and like influx DB or is just assistant for querying the data so that
maybe not anyway it is just query on the spot it is getting us the data and it will it will process it again it will be helpful to process but it can also process the time series data that you're telling. Okay, that's what I read about it. Okay. Yeah, but it will be able to. For example, if a like you said if we have saved any time series data in dB or any API, if it is returning a time series data, it will also be able to request that such API and also we can see the visualization.
And then the top right, right, the last six hours is there. Do you see that last six hours? Yeah. If you can change two days or three days. Oh, yeah. What are your seven days or so? Something is getting reflect.
I mean, I mean, I think we don't reflect anything because this is not kind of time series data. Right? It is kind of static data,
right? Correct. Correct. Yep. Yep. Yeah. Then it is good. Yeah. So far, no, maybe some time series data also we can no pull the data and then see that how it is being reflected. So that also may be maybe the next system we can consider that. Yeah, so one, okay. So let me summarize that. So what would the next is the year that whatever we are completed is using octane APA we are able to pull the data, pass it and then rendering it in year Grafana that we are able to do it three queries we are able to make we made it and then total test number of paths number of automated number of not automated test. So there are some discrepancies, you know which the correctness of the data at least for now, whatever the data, we are pulling it, we know what are the data and you checked also you are able to pull all the data, right, all that fields from the detail sections.
Yes, yes. Yeah. That, for example, here we have the numerous fields in this interactive VPN client, right? So whichever we feel we want, we can just, we can just add it in the query parameters.
Okay, okay. So you can we can pull all the data and then displaying it. So the only thing is now a time series data we are in our pull data and the display date. And third one is being stored in flux dB. And then we are see if you're able to showcase by using a time series data as well, without storing that without having to have influx dB. I don't know why we need influx dB. You know what I mean? So if you assume if you are sure that time series data also is possible, like you know, last two days, three days or one month data also, if you're able to know, displayed here, I don't know without data, how it will store that because there will be some kind of data.
Yeah, yeah, there should be some kind of data source I mean, this index Sorry, sorry, this infinity data source will be reading from the source, right. So that should be returning our time series data. So in turn, it can be IndexDB or any API that that itself, if it is if let's say nav is returning, so time series data, then it will vary that at last chart accordingly. Yeah, yeah. On the source, which which is from which it is actually getting the data? Sure.
So the next step would be like no correctness of the data, we just correct whatever that data is present in that new UI, and basically, everything will be correct. That's one correctness of the data will correct that and second one is maybe some time series data if you want to play it and then show it or however you want to proceed next.
Time please data from octane, I'm not sure how many if there is any use case, correct. You're right, you're right. Yeah, now I'm not able to think of anything proof. Yeah, maybe we can think of it and if there is anything we can connect.
Yeah. But we saw the correctness of data. We'll do that. That's one plus two point here.
I'm gonna go
on to sorry, you were saying something.
Yeah. So whatever still in this time, it is delta relative to the octane and not everything, thing or anything of that sort. Now, maybe if there is any use case we can try there. Or it's, we can proceed with know that we can like conclude that we are able to integrate, obtain data, JIRA and reports and steps to our graph and if all those things are feasible to check is done. And now we can start with requirements. We can come up with some concrete requirements and start implementing one by one
okay. Yeah, okay. So before we even start collecting the requirements, right. So what are the things we have finalized locked in we have finalized we are able to do that. And InfluxDB we can, we can. So there is an Excel sheet we have right
so that's there's no sharing it to you. Yeah, I mean, I was also thinking of the same and chatting it to you. Are you able to see it now?
Yes, I can see that. Yes. To
as far as what I've done, I have updated this. This I'm not this is the one that I'm having doubt on Grafana for that we have some SDKs and all but not sure of how we will be able to use it. I mean as of now we do not know any use case that is that band you need to be using this one. So accordingly we can dig deeper on this maybe put a separate spike later when we when we have to actually record it. And then sure continue.
Sure. And hello access to abusers and all other things we are done right multiple instance dashboard navigations this is all yes or no late so it's possible so you know that we can put it as yes
yeah, yeah. So so far we are done everything and then what we are. So with this, we are going to go with this Grafana right. And with this one we are going to do with the Grafana team. So something is not needed. Outlook is not needed. Slack is working. So that's fine. So we don't have to test it. Bestie already done that. So with these conclusions, we are just finalizing that no Grafana may be the tool to go ahead with this. That's one that's the maybe the final take away, but we will discuss with the sham also on this see what is seeing it as Okay, okay. And then we can come up with the requirements. Now next up would be like correctness of octane data and then solid requirements for at least one project to start with.
Correct? Correct. That should be the again start with any one project and depicted in response, and then we can continue the same for this. Yep. Okay. Yeah, maybe in that conversation, I will put up this this confluence page that I made correct. And maybe ask chomps review and then we can design Perfect, perfect.
Yep. Sounds good. Thank you, Monica. Anything else? So
